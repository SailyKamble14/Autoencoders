{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-08-19T06:51:57.359751Z",
     "iopub.status.busy": "2022-08-19T06:51:57.359310Z",
     "iopub.status.idle": "2022-08-19T06:51:57.364946Z",
     "shell.execute_reply": "2022-08-19T06:51:57.364193Z",
     "shell.execute_reply.started": "2022-08-19T06:51:57.359717Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "#for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    " #   for filename in filenames:\n",
    "  #      print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T07:19:24.467021Z",
     "iopub.status.busy": "2022-08-19T07:19:24.466642Z",
     "iopub.status.idle": "2022-08-19T07:19:24.476271Z",
     "shell.execute_reply": "2022-08-19T07:19:24.475189Z",
     "shell.execute_reply.started": "2022-08-19T07:19:24.466977Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.auto_encoder import AutoEncoder\n",
    "from pyod.utils.data import generate_data\n",
    "\n",
    "contamination = 0.1  # percentage of outliers\n",
    "n_train = 500  # number of training points\n",
    "n_test = 500  # number of testing points\n",
    "n_features = 25 # Number of features\n",
    "\n",
    "X_train,X_test,y_train,y_test = generate_data(\n",
    "    n_train=n_train, n_test=n_test,\n",
    "    n_features= n_features, \n",
    "    contamination=contamination,random_state=1234)\n",
    "\n",
    "X_train = pd.DataFrame(X_train)\n",
    "X_test = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T07:19:28.523387Z",
     "iopub.status.busy": "2022-08-19T07:19:28.522766Z",
     "iopub.status.idle": "2022-08-19T07:19:28.535756Z",
     "shell.execute_reply": "2022-08-19T07:19:28.534978Z",
     "shell.execute_reply.started": "2022-08-19T07:19:28.523330Z"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Sequential\n",
    "from  tensorflow.keras.layers import Dense, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "class Autoencoder_Large(Model):\n",
    "    def __init__(self, no_of_variables):\n",
    "        super(Autoencoder_Large, self).__init__()\n",
    "        self.OUTPUT_SIZE = no_of_variables\n",
    "        self.encoder = Sequential([\n",
    "            Dense(64, activation='relu'),\n",
    "            #Dense(64, activation='relu'),\n",
    "            #Dense(64, activation='relu'),\n",
    "            # Dropout(0.2),\n",
    "            Dense(64, activation='relu'),\n",
    "            # Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            # Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            # Dropout(0.4),\n",
    "            Dense(16, activation='relu'),\n",
    "            # Dropout(0.4),\n",
    "            Dense(16, activation='relu'),\n",
    "            # Dropout(0.4),\n",
    "            Dense(8, activation='relu'),\n",
    "            # Dropout(0.2),\n",
    "            Dense(8, activation='relu'),\n",
    "            Dense(4, activation='relu'),\n",
    "            # Dense(2, activation='relu'),\n",
    "\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        self.decoder = Sequential([\n",
    "            # Dense(1, activation='relu'),\n",
    "\n",
    "            Dense(4, activation='relu'),\n",
    "            Dense(8, activation='relu'),\n",
    "            Dense(8, activation='relu'),\n",
    "            # Dropout(0.2),\n",
    "            Dense(16, activation='relu'),\n",
    "            # Dropout(0.4),\n",
    "            Dense(16, activation='relu'),\n",
    "            # Dropout(0.4),\n",
    "            Dense(32, activation='relu'),\n",
    "            # Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            # Dropout(0.3),\n",
    "            Dense(64, activation='relu'),\n",
    "            # Dropout(0.2),\n",
    "            Dense(64, activation='relu'),\n",
    "            # # Dropout(0.2),\n",
    "            #Dense(64, activation='relu'), # removed for CPU\n",
    "            # Dropout(0.2),\n",
    "            #Dense(64, activation='relu'),\n",
    "            # Dropout(0.2),\n",
    "            Dense(32, activation='relu'),\n",
    "            # Dropout(0.2),\n",
    "            Dense(self.OUTPUT_SIZE, activation='linear')\n",
    "        ])\n",
    "\n",
    "    def call(self,x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T07:24:24.364687Z",
     "iopub.status.busy": "2022-08-19T07:24:24.364163Z",
     "iopub.status.idle": "2022-08-19T07:24:30.165719Z",
     "shell.execute_reply": "2022-08-19T07:24:30.164649Z",
     "shell.execute_reply.started": "2022-08-19T07:24:24.364651Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "4/4 [==============================] - 2s 78ms/step - loss: 2.8494 - val_loss: 2.8369\n",
      "Epoch 2/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.8427 - val_loss: 2.8317\n",
      "Epoch 3/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.8379 - val_loss: 2.8273\n",
      "Epoch 4/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.8336 - val_loss: 2.8232\n",
      "Epoch 5/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.8296 - val_loss: 2.8193\n",
      "Epoch 6/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.8257 - val_loss: 2.8156\n",
      "Epoch 7/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.8220 - val_loss: 2.8119\n",
      "Epoch 8/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.8183 - val_loss: 2.8082\n",
      "Epoch 9/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 2.8146 - val_loss: 2.8046\n",
      "Epoch 10/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.8110 - val_loss: 2.8009\n",
      "Epoch 11/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.8073 - val_loss: 2.7973\n",
      "Epoch 12/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.8037 - val_loss: 2.7937\n",
      "Epoch 13/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.8001 - val_loss: 2.7901\n",
      "Epoch 14/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.7965 - val_loss: 2.7865\n",
      "Epoch 15/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.7929 - val_loss: 2.7829\n",
      "Epoch 16/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.7893 - val_loss: 2.7793\n",
      "Epoch 17/500\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 2.7857 - val_loss: 2.7757\n",
      "Epoch 18/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 2.7821 - val_loss: 2.7721\n",
      "Epoch 19/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.7785 - val_loss: 2.7686\n",
      "Epoch 20/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.7749 - val_loss: 2.7650\n",
      "Epoch 21/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.7713 - val_loss: 2.7614\n",
      "Epoch 22/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 2.7677 - val_loss: 2.7578\n",
      "Epoch 23/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.7641 - val_loss: 2.7542\n",
      "Epoch 24/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.7605 - val_loss: 2.7506\n",
      "Epoch 25/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.7569 - val_loss: 2.7470\n",
      "Epoch 26/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.7533 - val_loss: 2.7435\n",
      "Epoch 27/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.7497 - val_loss: 2.7399\n",
      "Epoch 28/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.7461 - val_loss: 2.7363\n",
      "Epoch 29/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.7426 - val_loss: 2.7327\n",
      "Epoch 30/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.7390 - val_loss: 2.7292\n",
      "Epoch 31/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.7354 - val_loss: 2.7256\n",
      "Epoch 32/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.7318 - val_loss: 2.7220\n",
      "Epoch 33/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.7282 - val_loss: 2.7184\n",
      "Epoch 34/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.7246 - val_loss: 2.7149\n",
      "Epoch 35/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.7210 - val_loss: 2.7113\n",
      "Epoch 36/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.7174 - val_loss: 2.7077\n",
      "Epoch 37/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.7139 - val_loss: 2.7041\n",
      "Epoch 38/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.7103 - val_loss: 2.7005\n",
      "Epoch 39/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.7067 - val_loss: 2.6970\n",
      "Epoch 40/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.7031 - val_loss: 2.6934\n",
      "Epoch 41/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.6995 - val_loss: 2.6898\n",
      "Epoch 42/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.6959 - val_loss: 2.6862\n",
      "Epoch 43/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.6924 - val_loss: 2.6827\n",
      "Epoch 44/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.6888 - val_loss: 2.6791\n",
      "Epoch 45/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.6852 - val_loss: 2.6755\n",
      "Epoch 46/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.6816 - val_loss: 2.6719\n",
      "Epoch 47/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.6780 - val_loss: 2.6684\n",
      "Epoch 48/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.6744 - val_loss: 2.6648\n",
      "Epoch 49/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.6709 - val_loss: 2.6612\n",
      "Epoch 50/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.6673 - val_loss: 2.6576\n",
      "Epoch 51/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.6637 - val_loss: 2.6541\n",
      "Epoch 52/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.6601 - val_loss: 2.6505\n",
      "Epoch 53/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.6566 - val_loss: 2.6469\n",
      "Epoch 54/500\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 2.6530 - val_loss: 2.6434\n",
      "Epoch 55/500\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 2.6494 - val_loss: 2.6398\n",
      "Epoch 56/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.6458 - val_loss: 2.6362\n",
      "Epoch 57/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 2.6423 - val_loss: 2.6327\n",
      "Epoch 58/500\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 2.6387 - val_loss: 2.6291\n",
      "Epoch 59/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.6351 - val_loss: 2.6255\n",
      "Epoch 60/500\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 2.6315 - val_loss: 2.6220\n",
      "Epoch 61/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 2.6280 - val_loss: 2.6184\n",
      "Epoch 62/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 2.6244 - val_loss: 2.6148\n",
      "Epoch 63/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.6208 - val_loss: 2.6113\n",
      "Epoch 64/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.6173 - val_loss: 2.6077\n",
      "Epoch 65/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.6137 - val_loss: 2.6042\n",
      "Epoch 66/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.6102 - val_loss: 2.6006\n",
      "Epoch 67/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.6066 - val_loss: 2.5970\n",
      "Epoch 68/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.6030 - val_loss: 2.5935\n",
      "Epoch 69/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.5994 - val_loss: 2.5899\n",
      "Epoch 70/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.5959 - val_loss: 2.5864\n",
      "Epoch 71/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.5923 - val_loss: 2.5828\n",
      "Epoch 72/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.5887 - val_loss: 2.5792\n",
      "Epoch 73/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.5852 - val_loss: 2.5757\n",
      "Epoch 74/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.5816 - val_loss: 2.5721\n",
      "Epoch 75/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.5780 - val_loss: 2.5686\n",
      "Epoch 76/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.5745 - val_loss: 2.5650\n",
      "Epoch 77/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.5709 - val_loss: 2.5615\n",
      "Epoch 78/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.5674 - val_loss: 2.5579\n",
      "Epoch 79/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.5638 - val_loss: 2.5543\n",
      "Epoch 80/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.5602 - val_loss: 2.5508\n",
      "Epoch 81/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.5567 - val_loss: 2.5472\n",
      "Epoch 82/500\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 2.5531 - val_loss: 2.5437\n",
      "Epoch 83/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.5495 - val_loss: 2.5401\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.5460 - val_loss: 2.5366\n",
      "Epoch 85/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.5424 - val_loss: 2.5330\n",
      "Epoch 86/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.5389 - val_loss: 2.5295\n",
      "Epoch 87/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.5353 - val_loss: 2.5259\n",
      "Epoch 88/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.5318 - val_loss: 2.5223\n",
      "Epoch 89/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.5282 - val_loss: 2.5188\n",
      "Epoch 90/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.5247 - val_loss: 2.5152\n",
      "Epoch 91/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.5211 - val_loss: 2.5117\n",
      "Epoch 92/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.5176 - val_loss: 2.5081\n",
      "Epoch 93/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.5140 - val_loss: 2.5046\n",
      "Epoch 94/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.5105 - val_loss: 2.5010\n",
      "Epoch 95/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.5069 - val_loss: 2.4975\n",
      "Epoch 96/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.5034 - val_loss: 2.4939\n",
      "Epoch 97/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.4998 - val_loss: 2.4904\n",
      "Epoch 98/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.4963 - val_loss: 2.4868\n",
      "Epoch 99/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.4927 - val_loss: 2.4833\n",
      "Epoch 100/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.4892 - val_loss: 2.4797\n",
      "Epoch 101/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 2.4856 - val_loss: 2.4762\n",
      "Epoch 102/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.4821 - val_loss: 2.4726\n",
      "Epoch 103/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.4786 - val_loss: 2.4691\n",
      "Epoch 104/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.4750 - val_loss: 2.4656\n",
      "Epoch 105/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.4715 - val_loss: 2.4620\n",
      "Epoch 106/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.4679 - val_loss: 2.4585\n",
      "Epoch 107/500\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 2.4644 - val_loss: 2.4549\n",
      "Epoch 108/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 2.4608 - val_loss: 2.4514\n",
      "Epoch 109/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.4573 - val_loss: 2.4478\n",
      "Epoch 110/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.4537 - val_loss: 2.4442\n",
      "Epoch 111/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.4502 - val_loss: 2.4407\n",
      "Epoch 112/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.4467 - val_loss: 2.4372\n",
      "Epoch 113/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.4431 - val_loss: 2.4336\n",
      "Epoch 114/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.4396 - val_loss: 2.4301\n",
      "Epoch 115/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.4360 - val_loss: 2.4266\n",
      "Epoch 116/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.4325 - val_loss: 2.4230\n",
      "Epoch 117/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.4290 - val_loss: 2.4195\n",
      "Epoch 118/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.4254 - val_loss: 2.4160\n",
      "Epoch 119/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.4219 - val_loss: 2.4124\n",
      "Epoch 120/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.4184 - val_loss: 2.4089\n",
      "Epoch 121/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.4149 - val_loss: 2.4054\n",
      "Epoch 122/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.4113 - val_loss: 2.4018\n",
      "Epoch 123/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.4078 - val_loss: 2.3983\n",
      "Epoch 124/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.4042 - val_loss: 2.3948\n",
      "Epoch 125/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.4007 - val_loss: 2.3912\n",
      "Epoch 126/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.3972 - val_loss: 2.3877\n",
      "Epoch 127/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.3937 - val_loss: 2.3842\n",
      "Epoch 128/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.3901 - val_loss: 2.3806\n",
      "Epoch 129/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.3866 - val_loss: 2.3771\n",
      "Epoch 130/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.3831 - val_loss: 2.3736\n",
      "Epoch 131/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.3795 - val_loss: 2.3701\n",
      "Epoch 132/500\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 2.3760 - val_loss: 2.3666\n",
      "Epoch 133/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.3725 - val_loss: 2.3630\n",
      "Epoch 134/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.3690 - val_loss: 2.3595\n",
      "Epoch 135/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.3654 - val_loss: 2.3560\n",
      "Epoch 136/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.3619 - val_loss: 2.3525\n",
      "Epoch 137/500\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 2.3584 - val_loss: 2.3489\n",
      "Epoch 138/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 2.3549 - val_loss: 2.3454\n",
      "Epoch 139/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.3513 - val_loss: 2.3419\n",
      "Epoch 140/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.3478 - val_loss: 2.3384\n",
      "Epoch 141/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.3443 - val_loss: 2.3349\n",
      "Epoch 142/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.3408 - val_loss: 2.3314\n",
      "Epoch 143/500\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 2.3373 - val_loss: 2.3278\n",
      "Epoch 144/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.3338 - val_loss: 2.3243\n",
      "Epoch 145/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.3302 - val_loss: 2.3208\n",
      "Epoch 146/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.3267 - val_loss: 2.3173\n",
      "Epoch 147/500\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 2.3232 - val_loss: 2.3138\n",
      "Epoch 148/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.3197 - val_loss: 2.3102\n",
      "Epoch 149/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.3161 - val_loss: 2.3067\n",
      "Epoch 150/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.3126 - val_loss: 2.3032\n",
      "Epoch 151/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.3091 - val_loss: 2.2997\n",
      "Epoch 152/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.3056 - val_loss: 2.2962\n",
      "Epoch 153/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 2.3021 - val_loss: 2.2927\n",
      "Epoch 154/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.2986 - val_loss: 2.2892\n",
      "Epoch 155/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.2951 - val_loss: 2.2856\n",
      "Epoch 156/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.2915 - val_loss: 2.2821\n",
      "Epoch 157/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.2880 - val_loss: 2.2786\n",
      "Epoch 158/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.2845 - val_loss: 2.2751\n",
      "Epoch 159/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.2810 - val_loss: 2.2716\n",
      "Epoch 160/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.2775 - val_loss: 2.2681\n",
      "Epoch 161/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.2740 - val_loss: 2.2646\n",
      "Epoch 162/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.2705 - val_loss: 2.2611\n",
      "Epoch 163/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.2670 - val_loss: 2.2576\n",
      "Epoch 164/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.2635 - val_loss: 2.2541\n",
      "Epoch 165/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.2599 - val_loss: 2.2505\n",
      "Epoch 166/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 11ms/step - loss: 2.2564 - val_loss: 2.2470\n",
      "Epoch 167/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.2529 - val_loss: 2.2435\n",
      "Epoch 168/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.2494 - val_loss: 2.2400\n",
      "Epoch 169/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.2459 - val_loss: 2.2365\n",
      "Epoch 170/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.2424 - val_loss: 2.2331\n",
      "Epoch 171/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.2389 - val_loss: 2.2296\n",
      "Epoch 172/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.2354 - val_loss: 2.2261\n",
      "Epoch 173/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.2319 - val_loss: 2.2225\n",
      "Epoch 174/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 2.2284 - val_loss: 2.2190\n",
      "Epoch 175/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.2249 - val_loss: 2.2155\n",
      "Epoch 176/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.2214 - val_loss: 2.2120\n",
      "Epoch 177/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.2179 - val_loss: 2.2085\n",
      "Epoch 178/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.2144 - val_loss: 2.2050\n",
      "Epoch 179/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.2109 - val_loss: 2.2015\n",
      "Epoch 180/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.2074 - val_loss: 2.1980\n",
      "Epoch 181/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.2039 - val_loss: 2.1945\n",
      "Epoch 182/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.2004 - val_loss: 2.1911\n",
      "Epoch 183/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.1969 - val_loss: 2.1876\n",
      "Epoch 184/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.1934 - val_loss: 2.1841\n",
      "Epoch 185/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.1899 - val_loss: 2.1806\n",
      "Epoch 186/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.1864 - val_loss: 2.1771\n",
      "Epoch 187/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 2.1829 - val_loss: 2.1736\n",
      "Epoch 188/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.1795 - val_loss: 2.1701\n",
      "Epoch 189/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.1760 - val_loss: 2.1666\n",
      "Epoch 190/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.1725 - val_loss: 2.1631\n",
      "Epoch 191/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.1690 - val_loss: 2.1596\n",
      "Epoch 192/500\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 2.1655 - val_loss: 2.1561\n",
      "Epoch 193/500\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 2.1620 - val_loss: 2.1526\n",
      "Epoch 194/500\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 2.1586 - val_loss: 2.1492\n",
      "Epoch 195/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.1551 - val_loss: 2.1457\n",
      "Epoch 196/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.1516 - val_loss: 2.1422\n",
      "Epoch 197/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.1481 - val_loss: 2.1387\n",
      "Epoch 198/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.1446 - val_loss: 2.1352\n",
      "Epoch 199/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.1412 - val_loss: 2.1317\n",
      "Epoch 200/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.1377 - val_loss: 2.1282\n",
      "Epoch 201/500\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 2.1342 - val_loss: 2.1247\n",
      "Epoch 202/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.1307 - val_loss: 2.1212\n",
      "Epoch 203/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.1272 - val_loss: 2.1178\n",
      "Epoch 204/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.1238 - val_loss: 2.1143\n",
      "Epoch 205/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.1203 - val_loss: 2.1108\n",
      "Epoch 206/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.1168 - val_loss: 2.1073\n",
      "Epoch 207/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.1133 - val_loss: 2.1038\n",
      "Epoch 208/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.1098 - val_loss: 2.1004\n",
      "Epoch 209/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 2.1064 - val_loss: 2.0969\n",
      "Epoch 210/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.1029 - val_loss: 2.0934\n",
      "Epoch 211/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.0994 - val_loss: 2.0899\n",
      "Epoch 212/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.0959 - val_loss: 2.0864\n",
      "Epoch 213/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.0925 - val_loss: 2.0830\n",
      "Epoch 214/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 2.0890 - val_loss: 2.0795\n",
      "Epoch 215/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.0855 - val_loss: 2.0760\n",
      "Epoch 216/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.0820 - val_loss: 2.0725\n",
      "Epoch 217/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 2.0786 - val_loss: 2.0691\n",
      "Epoch 218/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.0751 - val_loss: 2.0656\n",
      "Epoch 219/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.0716 - val_loss: 2.0621\n",
      "Epoch 220/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 2.0682 - val_loss: 2.0586\n",
      "Epoch 221/500\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 2.0647 - val_loss: 2.0552\n",
      "Epoch 222/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.0612 - val_loss: 2.0517\n",
      "Epoch 223/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.0577 - val_loss: 2.0482\n",
      "Epoch 224/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.0543 - val_loss: 2.0447\n",
      "Epoch 225/500\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 2.0508 - val_loss: 2.0413\n",
      "Epoch 226/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.0473 - val_loss: 2.0378\n",
      "Epoch 227/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.0439 - val_loss: 2.0344\n",
      "Epoch 228/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.0404 - val_loss: 2.0309\n",
      "Epoch 229/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 2.0370 - val_loss: 2.0274\n",
      "Epoch 230/500\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 2.0335 - val_loss: 2.0240\n",
      "Epoch 231/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.0300 - val_loss: 2.0205\n",
      "Epoch 232/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.0266 - val_loss: 2.0170\n",
      "Epoch 233/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 2.0231 - val_loss: 2.0136\n",
      "Epoch 234/500\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 2.0197 - val_loss: 2.0101\n",
      "Epoch 235/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.0162 - val_loss: 2.0066\n",
      "Epoch 236/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 2.0127 - val_loss: 2.0032\n",
      "Epoch 237/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 2.0093 - val_loss: 1.9997\n",
      "Epoch 238/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 2.0058 - val_loss: 1.9962\n",
      "Epoch 239/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 2.0024 - val_loss: 1.9928\n",
      "Epoch 240/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.9989 - val_loss: 1.9893\n",
      "Epoch 241/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.9954 - val_loss: 1.9859\n",
      "Epoch 242/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.9920 - val_loss: 1.9824\n",
      "Epoch 243/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.9885 - val_loss: 1.9789\n",
      "Epoch 244/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.9851 - val_loss: 1.9755\n",
      "Epoch 245/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.9816 - val_loss: 1.9720\n",
      "Epoch 246/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.9782 - val_loss: 1.9686\n",
      "Epoch 247/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.9747 - val_loss: 1.9651\n",
      "Epoch 248/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 14ms/step - loss: 1.9713 - val_loss: 1.9616\n",
      "Epoch 249/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.9678 - val_loss: 1.9582\n",
      "Epoch 250/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.9644 - val_loss: 1.9547\n",
      "Epoch 251/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.9609 - val_loss: 1.9513\n",
      "Epoch 252/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.9575 - val_loss: 1.9479\n",
      "Epoch 253/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.9541 - val_loss: 1.9444\n",
      "Epoch 254/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.9506 - val_loss: 1.9409\n",
      "Epoch 255/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.9472 - val_loss: 1.9375\n",
      "Epoch 256/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.9437 - val_loss: 1.9341\n",
      "Epoch 257/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.9403 - val_loss: 1.9306\n",
      "Epoch 258/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.9368 - val_loss: 1.9272\n",
      "Epoch 259/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.9334 - val_loss: 1.9237\n",
      "Epoch 260/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.9300 - val_loss: 1.9203\n",
      "Epoch 261/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.9265 - val_loss: 1.9169\n",
      "Epoch 262/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.9231 - val_loss: 1.9134\n",
      "Epoch 263/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.9196 - val_loss: 1.9099\n",
      "Epoch 264/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.9162 - val_loss: 1.9065\n",
      "Epoch 265/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.9128 - val_loss: 1.9030\n",
      "Epoch 266/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.9093 - val_loss: 1.8996\n",
      "Epoch 267/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.9059 - val_loss: 1.8962\n",
      "Epoch 268/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.9025 - val_loss: 1.8927\n",
      "Epoch 269/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.8990 - val_loss: 1.8893\n",
      "Epoch 270/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.8956 - val_loss: 1.8859\n",
      "Epoch 271/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 1.8922 - val_loss: 1.8824\n",
      "Epoch 272/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.8887 - val_loss: 1.8790\n",
      "Epoch 273/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.8853 - val_loss: 1.8755\n",
      "Epoch 274/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.8818 - val_loss: 1.8721\n",
      "Epoch 275/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.8784 - val_loss: 1.8686\n",
      "Epoch 276/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.8750 - val_loss: 1.8652\n",
      "Epoch 277/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.8716 - val_loss: 1.8618\n",
      "Epoch 278/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.8681 - val_loss: 1.8583\n",
      "Epoch 279/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.8647 - val_loss: 1.8549\n",
      "Epoch 280/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.8613 - val_loss: 1.8515\n",
      "Epoch 281/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.8579 - val_loss: 1.8481\n",
      "Epoch 282/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.8544 - val_loss: 1.8446\n",
      "Epoch 283/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.8510 - val_loss: 1.8412\n",
      "Epoch 284/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.8476 - val_loss: 1.8378\n",
      "Epoch 285/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.8442 - val_loss: 1.8343\n",
      "Epoch 286/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.8407 - val_loss: 1.8309\n",
      "Epoch 287/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.8373 - val_loss: 1.8275\n",
      "Epoch 288/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 1.8339 - val_loss: 1.8241\n",
      "Epoch 289/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.8305 - val_loss: 1.8206\n",
      "Epoch 290/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.8271 - val_loss: 1.8172\n",
      "Epoch 291/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.8237 - val_loss: 1.8138\n",
      "Epoch 292/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.8203 - val_loss: 1.8104\n",
      "Epoch 293/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.8168 - val_loss: 1.8069\n",
      "Epoch 294/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.8134 - val_loss: 1.8035\n",
      "Epoch 295/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.8100 - val_loss: 1.8001\n",
      "Epoch 296/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.8066 - val_loss: 1.7967\n",
      "Epoch 297/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.8032 - val_loss: 1.7933\n",
      "Epoch 298/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.7998 - val_loss: 1.7899\n",
      "Epoch 299/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.7964 - val_loss: 1.7864\n",
      "Epoch 300/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.7930 - val_loss: 1.7830\n",
      "Epoch 301/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.7896 - val_loss: 1.7796\n",
      "Epoch 302/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 1.7861 - val_loss: 1.7762\n",
      "Epoch 303/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 1.7827 - val_loss: 1.7728\n",
      "Epoch 304/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.7793 - val_loss: 1.7694\n",
      "Epoch 305/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.7759 - val_loss: 1.7660\n",
      "Epoch 306/500\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 1.7725 - val_loss: 1.7626\n",
      "Epoch 307/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.7691 - val_loss: 1.7592\n",
      "Epoch 308/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.7657 - val_loss: 1.7557\n",
      "Epoch 309/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.7623 - val_loss: 1.7523\n",
      "Epoch 310/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.7589 - val_loss: 1.7489\n",
      "Epoch 311/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.7555 - val_loss: 1.7455\n",
      "Epoch 312/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.7521 - val_loss: 1.7421\n",
      "Epoch 313/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.7487 - val_loss: 1.7387\n",
      "Epoch 314/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.7453 - val_loss: 1.7353\n",
      "Epoch 315/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.7419 - val_loss: 1.7319\n",
      "Epoch 316/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.7385 - val_loss: 1.7285\n",
      "Epoch 317/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.7351 - val_loss: 1.7251\n",
      "Epoch 318/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.7317 - val_loss: 1.7217\n",
      "Epoch 319/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.7283 - val_loss: 1.7183\n",
      "Epoch 320/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.7249 - val_loss: 1.7149\n",
      "Epoch 321/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.7215 - val_loss: 1.7115\n",
      "Epoch 322/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.7181 - val_loss: 1.7082\n",
      "Epoch 323/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.7148 - val_loss: 1.7048\n",
      "Epoch 324/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.7114 - val_loss: 1.7014\n",
      "Epoch 325/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.7080 - val_loss: 1.6980\n",
      "Epoch 326/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.7046 - val_loss: 1.6946\n",
      "Epoch 327/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.7012 - val_loss: 1.6912\n",
      "Epoch 328/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.6978 - val_loss: 1.6878\n",
      "Epoch 329/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.6944 - val_loss: 1.6845\n",
      "Epoch 330/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 14ms/step - loss: 1.6911 - val_loss: 1.6811\n",
      "Epoch 331/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.6877 - val_loss: 1.6777\n",
      "Epoch 332/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.6843 - val_loss: 1.6743\n",
      "Epoch 333/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.6809 - val_loss: 1.6709\n",
      "Epoch 334/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.6775 - val_loss: 1.6675\n",
      "Epoch 335/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.6741 - val_loss: 1.6641\n",
      "Epoch 336/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.6708 - val_loss: 1.6608\n",
      "Epoch 337/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.6674 - val_loss: 1.6574\n",
      "Epoch 338/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.6640 - val_loss: 1.6540\n",
      "Epoch 339/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.6607 - val_loss: 1.6506\n",
      "Epoch 340/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.6573 - val_loss: 1.6473\n",
      "Epoch 341/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.6539 - val_loss: 1.6439\n",
      "Epoch 342/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.6505 - val_loss: 1.6405\n",
      "Epoch 343/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.6472 - val_loss: 1.6371\n",
      "Epoch 344/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.6438 - val_loss: 1.6337\n",
      "Epoch 345/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.6404 - val_loss: 1.6304\n",
      "Epoch 346/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.6371 - val_loss: 1.6270\n",
      "Epoch 347/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.6337 - val_loss: 1.6236\n",
      "Epoch 348/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.6303 - val_loss: 1.6203\n",
      "Epoch 349/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.6270 - val_loss: 1.6169\n",
      "Epoch 350/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.6236 - val_loss: 1.6136\n",
      "Epoch 351/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 1.6203 - val_loss: 1.6102\n",
      "Epoch 352/500\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 1.6169 - val_loss: 1.6068\n",
      "Epoch 353/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.6136 - val_loss: 1.6034\n",
      "Epoch 354/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.6102 - val_loss: 1.6001\n",
      "Epoch 355/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.6068 - val_loss: 1.5967\n",
      "Epoch 356/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.6035 - val_loss: 1.5933\n",
      "Epoch 357/500\n",
      "4/4 [==============================] - 0s 21ms/step - loss: 1.6002 - val_loss: 1.5900\n",
      "Epoch 358/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 1.5968 - val_loss: 1.5866\n",
      "Epoch 359/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.5935 - val_loss: 1.5832\n",
      "Epoch 360/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 1.5901 - val_loss: 1.5799\n",
      "Epoch 361/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.5868 - val_loss: 1.5765\n",
      "Epoch 362/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 1.5834 - val_loss: 1.5732\n",
      "Epoch 363/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.5801 - val_loss: 1.5698\n",
      "Epoch 364/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.5767 - val_loss: 1.5664\n",
      "Epoch 365/500\n",
      "4/4 [==============================] - 0s 25ms/step - loss: 1.5734 - val_loss: 1.5631\n",
      "Epoch 366/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 1.5701 - val_loss: 1.5597\n",
      "Epoch 367/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.5667 - val_loss: 1.5564\n",
      "Epoch 368/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.5634 - val_loss: 1.5530\n",
      "Epoch 369/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.5600 - val_loss: 1.5497\n",
      "Epoch 370/500\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 1.5567 - val_loss: 1.5464\n",
      "Epoch 371/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.5534 - val_loss: 1.5430\n",
      "Epoch 372/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.5501 - val_loss: 1.5397\n",
      "Epoch 373/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 1.5467 - val_loss: 1.5363\n",
      "Epoch 374/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.5434 - val_loss: 1.5330\n",
      "Epoch 375/500\n",
      "4/4 [==============================] - 0s 18ms/step - loss: 1.5401 - val_loss: 1.5297\n",
      "Epoch 376/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.5368 - val_loss: 1.5263\n",
      "Epoch 377/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.5335 - val_loss: 1.5230\n",
      "Epoch 378/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.5301 - val_loss: 1.5197\n",
      "Epoch 379/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.5268 - val_loss: 1.5163\n",
      "Epoch 380/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.5235 - val_loss: 1.5130\n",
      "Epoch 381/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.5202 - val_loss: 1.5097\n",
      "Epoch 382/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.5169 - val_loss: 1.5064\n",
      "Epoch 383/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.5136 - val_loss: 1.5031\n",
      "Epoch 384/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.5103 - val_loss: 1.4997\n",
      "Epoch 385/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.5070 - val_loss: 1.4964\n",
      "Epoch 386/500\n",
      "4/4 [==============================] - 0s 24ms/step - loss: 1.5037 - val_loss: 1.4931\n",
      "Epoch 387/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.5004 - val_loss: 1.4898\n",
      "Epoch 388/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.4971 - val_loss: 1.4864\n",
      "Epoch 389/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.4938 - val_loss: 1.4831\n",
      "Epoch 390/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.4905 - val_loss: 1.4798\n",
      "Epoch 391/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.4872 - val_loss: 1.4765\n",
      "Epoch 392/500\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1.4839 - val_loss: 1.4732\n",
      "Epoch 393/500\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 1.4806 - val_loss: 1.4699\n",
      "Epoch 394/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.4773 - val_loss: 1.4666\n",
      "Epoch 395/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.4740 - val_loss: 1.4633\n",
      "Epoch 396/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.4707 - val_loss: 1.4600\n",
      "Epoch 397/500\n",
      "4/4 [==============================] - 0s 22ms/step - loss: 1.4674 - val_loss: 1.4567\n",
      "Epoch 398/500\n",
      "4/4 [==============================] - 0s 20ms/step - loss: 1.4641 - val_loss: 1.4534\n",
      "Epoch 399/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 1.4609 - val_loss: 1.4501\n",
      "Epoch 400/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.4576 - val_loss: 1.4468\n",
      "Epoch 401/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 1.4543 - val_loss: 1.4435\n",
      "Epoch 402/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.4510 - val_loss: 1.4402\n",
      "Epoch 403/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.4478 - val_loss: 1.4369\n",
      "Epoch 404/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.4445 - val_loss: 1.4336\n",
      "Epoch 405/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.4412 - val_loss: 1.4304\n",
      "Epoch 406/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.4379 - val_loss: 1.4271\n",
      "Epoch 407/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.4347 - val_loss: 1.4238\n",
      "Epoch 408/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.4314 - val_loss: 1.4205\n",
      "Epoch 409/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.4282 - val_loss: 1.4173\n",
      "Epoch 410/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.4249 - val_loss: 1.4140\n",
      "Epoch 411/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.4217 - val_loss: 1.4107\n",
      "Epoch 412/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 9ms/step - loss: 1.4185 - val_loss: 1.4075\n",
      "Epoch 413/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.4152 - val_loss: 1.4042\n",
      "Epoch 414/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.4119 - val_loss: 1.4009\n",
      "Epoch 415/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.4087 - val_loss: 1.3977\n",
      "Epoch 416/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.4055 - val_loss: 1.3944\n",
      "Epoch 417/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.4022 - val_loss: 1.3912\n",
      "Epoch 418/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.3990 - val_loss: 1.3879\n",
      "Epoch 419/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.3958 - val_loss: 1.3847\n",
      "Epoch 420/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.3925 - val_loss: 1.3814\n",
      "Epoch 421/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.3893 - val_loss: 1.3782\n",
      "Epoch 422/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.3861 - val_loss: 1.3749\n",
      "Epoch 423/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.3829 - val_loss: 1.3717\n",
      "Epoch 424/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.3796 - val_loss: 1.3685\n",
      "Epoch 425/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.3764 - val_loss: 1.3652\n",
      "Epoch 426/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.3732 - val_loss: 1.3620\n",
      "Epoch 427/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.3700 - val_loss: 1.3588\n",
      "Epoch 428/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.3668 - val_loss: 1.3555\n",
      "Epoch 429/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.3636 - val_loss: 1.3523\n",
      "Epoch 430/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.3604 - val_loss: 1.3491\n",
      "Epoch 431/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.3572 - val_loss: 1.3459\n",
      "Epoch 432/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.3540 - val_loss: 1.3427\n",
      "Epoch 433/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 1.3508 - val_loss: 1.3395\n",
      "Epoch 434/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.3476 - val_loss: 1.3362\n",
      "Epoch 435/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.3444 - val_loss: 1.3330\n",
      "Epoch 436/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.3412 - val_loss: 1.3298\n",
      "Epoch 437/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.3381 - val_loss: 1.3266\n",
      "Epoch 438/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.3349 - val_loss: 1.3234\n",
      "Epoch 439/500\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 1.3317 - val_loss: 1.3202\n",
      "Epoch 440/500\n",
      "4/4 [==============================] - 0s 23ms/step - loss: 1.3285 - val_loss: 1.3170\n",
      "Epoch 441/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.3254 - val_loss: 1.3138\n",
      "Epoch 442/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.3222 - val_loss: 1.3106\n",
      "Epoch 443/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.3190 - val_loss: 1.3075\n",
      "Epoch 444/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.3159 - val_loss: 1.3043\n",
      "Epoch 445/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.3128 - val_loss: 1.3011\n",
      "Epoch 446/500\n",
      "4/4 [==============================] - 0s 8ms/step - loss: 1.3096 - val_loss: 1.2979\n",
      "Epoch 447/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.3064 - val_loss: 1.2948\n",
      "Epoch 448/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.3033 - val_loss: 1.2916\n",
      "Epoch 449/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.3001 - val_loss: 1.2884\n",
      "Epoch 450/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.2970 - val_loss: 1.2853\n",
      "Epoch 451/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.2939 - val_loss: 1.2821\n",
      "Epoch 452/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.2907 - val_loss: 1.2790\n",
      "Epoch 453/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.2876 - val_loss: 1.2758\n",
      "Epoch 454/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.2845 - val_loss: 1.2727\n",
      "Epoch 455/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.2813 - val_loss: 1.2695\n",
      "Epoch 456/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.2782 - val_loss: 1.2664\n",
      "Epoch 457/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.2751 - val_loss: 1.2633\n",
      "Epoch 458/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.2720 - val_loss: 1.2602\n",
      "Epoch 459/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.2689 - val_loss: 1.2570\n",
      "Epoch 460/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.2658 - val_loss: 1.2539\n",
      "Epoch 461/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.2627 - val_loss: 1.2508\n",
      "Epoch 462/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.2596 - val_loss: 1.2477\n",
      "Epoch 463/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.2565 - val_loss: 1.2446\n",
      "Epoch 464/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.2534 - val_loss: 1.2415\n",
      "Epoch 465/500\n",
      "4/4 [==============================] - 0s 17ms/step - loss: 1.2503 - val_loss: 1.2384\n",
      "Epoch 466/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.2473 - val_loss: 1.2353\n",
      "Epoch 467/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.2442 - val_loss: 1.2322\n",
      "Epoch 468/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.2411 - val_loss: 1.2291\n",
      "Epoch 469/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.2380 - val_loss: 1.2261\n",
      "Epoch 470/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.2350 - val_loss: 1.2230\n",
      "Epoch 471/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.2319 - val_loss: 1.2200\n",
      "Epoch 472/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.2289 - val_loss: 1.2169\n",
      "Epoch 473/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.2258 - val_loss: 1.2138\n",
      "Epoch 474/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.2228 - val_loss: 1.2108\n",
      "Epoch 475/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.2197 - val_loss: 1.2077\n",
      "Epoch 476/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.2167 - val_loss: 1.2047\n",
      "Epoch 477/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.2137 - val_loss: 1.2016\n",
      "Epoch 478/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 1.2106 - val_loss: 1.1986\n",
      "Epoch 479/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.2076 - val_loss: 1.1955\n",
      "Epoch 480/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.2046 - val_loss: 1.1925\n",
      "Epoch 481/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.2016 - val_loss: 1.1895\n",
      "Epoch 482/500\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 1.1986 - val_loss: 1.1865\n",
      "Epoch 483/500\n",
      "4/4 [==============================] - 0s 19ms/step - loss: 1.1956 - val_loss: 1.1835\n",
      "Epoch 484/500\n",
      "4/4 [==============================] - 0s 15ms/step - loss: 1.1926 - val_loss: 1.1805\n",
      "Epoch 485/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.1896 - val_loss: 1.1775\n",
      "Epoch 486/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.1867 - val_loss: 1.1745\n",
      "Epoch 487/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.1837 - val_loss: 1.1715\n",
      "Epoch 488/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.1807 - val_loss: 1.1685\n",
      "Epoch 489/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.1777 - val_loss: 1.1655\n",
      "Epoch 490/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.1748 - val_loss: 1.1625\n",
      "Epoch 491/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.1718 - val_loss: 1.1595\n",
      "Epoch 492/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.1688 - val_loss: 1.1565\n",
      "Epoch 493/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.1659 - val_loss: 1.1536\n",
      "Epoch 494/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 11ms/step - loss: 1.1629 - val_loss: 1.1506\n",
      "Epoch 495/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.1600 - val_loss: 1.1476\n",
      "Epoch 496/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.1570 - val_loss: 1.1447\n",
      "Epoch 497/500\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 1.1541 - val_loss: 1.1418\n",
      "Epoch 498/500\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 1.1512 - val_loss: 1.1389\n",
      "Epoch 499/500\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 1.1482 - val_loss: 1.1359\n",
      "Epoch 500/500\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 1.1453 - val_loss: 1.1330\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, TensorBoard\n",
    "early_stopping = EarlyStopping(monitor='val_loss',\n",
    "                                   patience = 10,\n",
    "                                   mode = 'min',restore_best_weights=True)\n",
    "    # Compiling\n",
    "model = Autoencoder_Large(25)\n",
    "optim = keras.optimizers.RMSprop()\n",
    "model.compile(optimizer = optim,\n",
    "                  loss = 'mae')\n",
    "\n",
    "    # Training the model\n",
    "history = model.fit(X_train,X_train,epochs = 500,batch_size = 128,validation_data = (X_test,X_test),callbacks = [early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T07:25:45.660830Z",
     "iopub.status.busy": "2022-08-19T07:25:45.660406Z",
     "iopub.status.idle": "2022-08-19T07:25:45.744674Z",
     "shell.execute_reply": "2022-08-19T07:25:45.743243Z",
     "shell.execute_reply.started": "2022-08-19T07:25:45.660788Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE  0.1\n",
      "MAE  0.1\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred,1)\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "print(\"MSE \",mean_squared_error(y_test,y_pred))\n",
    "print(\"MAE \",mean_absolute_error(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T13:11:28.318928Z",
     "iopub.status.busy": "2022-08-18T13:11:28.318477Z",
     "iopub.status.idle": "2022-08-18T13:11:37.317627Z",
     "shell.execute_reply": "2022-08-18T13:11:37.316378Z",
     "shell.execute_reply.started": "2022-08-18T13:11:28.318889Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_74 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_29 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_75 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_30 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_76 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_31 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_77 (Dense)            (None, 64)                1664      \n",
      "                                                                 \n",
      " dropout_32 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_78 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_33 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_79 (Dense)            (None, 24)                792       \n",
      "                                                                 \n",
      " dropout_34 (Dropout)        (None, 24)                0         \n",
      "                                                                 \n",
      " dense_80 (Dense)            (None, 10)                250       \n",
      "                                                                 \n",
      " dropout_35 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_81 (Dense)            (None, 2)                 22        \n",
      "                                                                 \n",
      " dropout_36 (Dropout)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_82 (Dense)            (None, 10)                30        \n",
      "                                                                 \n",
      " dropout_37 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_83 (Dense)            (None, 24)                264       \n",
      "                                                                 \n",
      " dropout_38 (Dropout)        (None, 24)                0         \n",
      "                                                                 \n",
      " dense_84 (Dense)            (None, 32)                800       \n",
      "                                                                 \n",
      " dropout_39 (Dropout)        (None, 32)                0         \n",
      "                                                                 \n",
      " dense_85 (Dense)            (None, 64)                2112      \n",
      "                                                                 \n",
      " dropout_40 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_86 (Dense)            (None, 25)                1625      \n",
      "                                                                 \n",
      " dropout_41 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_87 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 12,239\n",
      "Trainable params: 12,239\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/50\n",
      "15/15 [==============================] - 2s 36ms/step - loss: 3.5224 - val_loss: 4.1933\n",
      "Epoch 2/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.7986 - val_loss: 3.6266\n",
      "Epoch 3/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2.4500 - val_loss: 3.2752\n",
      "Epoch 4/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.2133 - val_loss: 3.0188\n",
      "Epoch 5/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.0327 - val_loss: 2.8254\n",
      "Epoch 6/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.9004 - val_loss: 2.6733\n",
      "Epoch 7/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.7830 - val_loss: 2.5469\n",
      "Epoch 8/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.7136 - val_loss: 2.4555\n",
      "Epoch 9/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.6448 - val_loss: 2.3856\n",
      "Epoch 10/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.5875 - val_loss: 2.3338\n",
      "Epoch 11/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.5471 - val_loss: 2.2756\n",
      "Epoch 12/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.5053 - val_loss: 2.2419\n",
      "Epoch 13/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.4743 - val_loss: 2.2023\n",
      "Epoch 14/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.4398 - val_loss: 2.1716\n",
      "Epoch 15/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.4142 - val_loss: 2.1409\n",
      "Epoch 16/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.3915 - val_loss: 2.1172\n",
      "Epoch 17/50\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.3714 - val_loss: 2.0919\n",
      "Epoch 18/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.3479 - val_loss: 2.0678\n",
      "Epoch 19/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.3294 - val_loss: 2.0496\n",
      "Epoch 20/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.3100 - val_loss: 2.0251\n",
      "Epoch 21/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.2952 - val_loss: 2.0050\n",
      "Epoch 22/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2816 - val_loss: 1.9842\n",
      "Epoch 23/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2624 - val_loss: 1.9724\n",
      "Epoch 24/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2477 - val_loss: 1.9522\n",
      "Epoch 25/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2382 - val_loss: 1.9400\n",
      "Epoch 26/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2256 - val_loss: 1.9261\n",
      "Epoch 27/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2122 - val_loss: 1.9116\n",
      "Epoch 28/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2020 - val_loss: 1.9014\n",
      "Epoch 29/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1937 - val_loss: 1.8889\n",
      "Epoch 30/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.1870 - val_loss: 1.8770\n",
      "Epoch 31/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1761 - val_loss: 1.8671\n",
      "Epoch 32/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.1682 - val_loss: 1.8534\n",
      "Epoch 33/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1566 - val_loss: 1.8490\n",
      "Epoch 34/50\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.1497 - val_loss: 1.8369\n",
      "Epoch 35/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.1453 - val_loss: 1.8284\n",
      "Epoch 36/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.1374 - val_loss: 1.8208\n",
      "Epoch 37/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1304 - val_loss: 1.8125\n",
      "Epoch 38/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.1260 - val_loss: 1.8072\n",
      "Epoch 39/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1221 - val_loss: 1.7979\n",
      "Epoch 40/50\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.1137 - val_loss: 1.7898\n",
      "Epoch 41/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1082 - val_loss: 1.7864\n",
      "Epoch 42/50\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.1051 - val_loss: 1.7784\n",
      "Epoch 43/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1017 - val_loss: 1.7714\n",
      "Epoch 44/50\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0981 - val_loss: 1.7647\n",
      "Epoch 45/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0966 - val_loss: 1.7581\n",
      "Epoch 46/50\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.0909 - val_loss: 1.7587\n",
      "Epoch 47/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0865 - val_loss: 1.7507\n",
      "Epoch 48/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0833 - val_loss: 1.7470\n",
      "Epoch 49/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0797 - val_loss: 1.7438\n",
      "Epoch 50/50\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0766 - val_loss: 1.7362\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AutoEncoder(batch_size=32, contamination=0.1, dropout_rate=0.2, epochs=50,\n",
       "      hidden_activation='relu',\n",
       "      hidden_neurons=[25, 64, 32, 24, 10, 2, 10, 24, 32, 64, 25],\n",
       "      l2_regularizer=0.1,\n",
       "      loss=<function mean_squared_error at 0x000002C327973EE0>,\n",
       "      optimizer='adam', output_activation='sigmoid', preprocessing=True,\n",
       "      random_state=None, validation_size=0.1, verbose=1)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### PYOD model 1 ####\n",
    "clf1 = AutoEncoder(hidden_neurons =[25,64,32,24,10,2,10,24,32,64, 25],epochs=50,verbose=1)\n",
    "clf1.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-19T07:10:34.857890Z",
     "iopub.status.busy": "2022-08-19T07:10:34.857361Z",
     "iopub.status.idle": "2022-08-19T07:10:34.888151Z",
     "shell.execute_reply": "2022-08-19T07:10:34.886216Z",
     "shell.execute_reply.started": "2022-08-19T07:10:34.857846Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the outlier scores for the train data\n",
    "y_train_scores = clf1.decision_scores_  \n",
    "\n",
    "# Predict the anomaly scores\n",
    "y_test_scores = clf1.decision_function(X_test)  # outlier scores\n",
    "y_test_scores = pd.Series(y_test_scores)\n",
    "\n",
    "# Plot it!\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_test_scores, bins='auto')  \n",
    "plt.title(\"Histogram for Model Clf1 Anomaly Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T11:40:27.499702Z",
     "iopub.status.busy": "2021-07-15T11:40:27.499312Z",
     "iopub.status.idle": "2021-07-15T11:40:27.506507Z",
     "shell.execute_reply": "2021-07-15T11:40:27.50532Z",
     "shell.execute_reply.started": "2021-07-15T11:40:27.499673Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_scores.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We choose 4.0 to be the cut point and those >=4.0 to be outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T11:43:28.329356Z",
     "iopub.status.busy": "2021-07-15T11:43:28.328924Z",
     "iopub.status.idle": "2021-07-15T11:43:28.383595Z",
     "shell.execute_reply": "2021-07-15T11:43:28.382897Z",
     "shell.execute_reply.started": "2021-07-15T11:43:28.329314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.038355</td>\n",
       "      <td>2.969163</td>\n",
       "      <td>2.995461</td>\n",
       "      <td>3.003440</td>\n",
       "      <td>2.981764</td>\n",
       "      <td>2.986800</td>\n",
       "      <td>2.992153</td>\n",
       "      <td>2.970264</td>\n",
       "      <td>3.059222</td>\n",
       "      <td>2.996335</td>\n",
       "      <td>...</td>\n",
       "      <td>3.015313</td>\n",
       "      <td>3.007682</td>\n",
       "      <td>3.060115</td>\n",
       "      <td>2.993295</td>\n",
       "      <td>2.946253</td>\n",
       "      <td>3.001370</td>\n",
       "      <td>3.028487</td>\n",
       "      <td>2.971585</td>\n",
       "      <td>3.001477</td>\n",
       "      <td>2.666125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.129767</td>\n",
       "      <td>0.231696</td>\n",
       "      <td>-0.225120</td>\n",
       "      <td>0.273774</td>\n",
       "      <td>0.228626</td>\n",
       "      <td>-0.125085</td>\n",
       "      <td>-0.139579</td>\n",
       "      <td>0.282078</td>\n",
       "      <td>-0.057665</td>\n",
       "      <td>-0.120514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111293</td>\n",
       "      <td>-0.060746</td>\n",
       "      <td>-0.082725</td>\n",
       "      <td>-0.036849</td>\n",
       "      <td>0.394688</td>\n",
       "      <td>0.339425</td>\n",
       "      <td>0.250229</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>-0.093264</td>\n",
       "      <td>13.593021</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "cluster                                                                         \n",
       "0        3.038355  2.969163  2.995461  3.003440  2.981764  2.986800  2.992153   \n",
       "1        0.129767  0.231696 -0.225120  0.273774  0.228626 -0.125085 -0.139579   \n",
       "\n",
       "                7         8         9  ...        16        17        18  \\\n",
       "cluster                                ...                                 \n",
       "0        2.970264  3.059222  2.996335  ...  3.015313  3.007682  3.060115   \n",
       "1        0.282078 -0.057665 -0.120514  ...  0.111293 -0.060746 -0.082725   \n",
       "\n",
       "               19        20        21        22        23        24      score  \n",
       "cluster                                                                         \n",
       "0        2.993295  2.946253  3.001370  3.028487  2.971585  3.001477   2.666125  \n",
       "1       -0.036849  0.394688  0.339425  0.250229  0.035533 -0.093264  13.593021  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = X_test.copy()\n",
    "df_test['score'] = y_test_scores\n",
    "df_test['cluster'] = np.where(df_test['score']<4, 0, 1)\n",
    "df_test['cluster'].value_counts()\n",
    "\n",
    "df_test.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T11:43:54.329984Z",
     "iopub.status.busy": "2021-07-15T11:43:54.329467Z",
     "iopub.status.idle": "2021-07-15T11:43:54.334772Z",
     "shell.execute_reply": "2021-07-15T11:43:54.334194Z",
     "shell.execute_reply.started": "2021-07-15T11:43:54.329955Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 27)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### PYOD model 2 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-18T12:10:10.180817Z",
     "iopub.status.busy": "2022-08-18T12:10:10.180324Z",
     "iopub.status.idle": "2022-08-18T12:10:23.739458Z",
     "shell.execute_reply": "2022-08-18T12:10:23.737687Z",
     "shell.execute_reply.started": "2022-08-18T12:10:10.180767Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_88 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_42 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_89 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_43 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_90 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_44 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_91 (Dense)            (None, 10)                260       \n",
      "                                                                 \n",
      " dropout_45 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_92 (Dense)            (None, 2)                 22        \n",
      "                                                                 \n",
      " dropout_46 (Dropout)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_93 (Dense)            (None, 10)                30        \n",
      "                                                                 \n",
      " dropout_47 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_94 (Dense)            (None, 25)                275       \n",
      "                                                                 \n",
      " dropout_48 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_95 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,187\n",
      "Trainable params: 3,187\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 1s 22ms/step - loss: 4.4813 - val_loss: 4.6237\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 3.5798 - val_loss: 3.8827\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.0288 - val_loss: 3.4059\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2.6718 - val_loss: 3.0973\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2.4403 - val_loss: 2.8798\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2.2463 - val_loss: 2.6704\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.0927 - val_loss: 2.5035\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.9632 - val_loss: 2.3646\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.8527 - val_loss: 2.2560\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.7646 - val_loss: 2.1667\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.6826 - val_loss: 2.0813\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.6371 - val_loss: 2.0223\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.5838 - val_loss: 1.9763\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.5387 - val_loss: 1.9342\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.5061 - val_loss: 1.8989\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.4710 - val_loss: 1.8662\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.4482 - val_loss: 1.8333\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.4186 - val_loss: 1.8087\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.3982 - val_loss: 1.7849\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.3847 - val_loss: 1.7653\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.3613 - val_loss: 1.7456\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.3469 - val_loss: 1.7275\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.3305 - val_loss: 1.7083\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.3086 - val_loss: 1.6926\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.2978 - val_loss: 1.6770\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2860 - val_loss: 1.6613\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2745 - val_loss: 1.6470\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2605 - val_loss: 1.6363\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2500 - val_loss: 1.6226\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.2377 - val_loss: 1.6128\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2306 - val_loss: 1.5999\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2211 - val_loss: 1.5897\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2173 - val_loss: 1.5811\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2083 - val_loss: 1.5726\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2002 - val_loss: 1.5647\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1921 - val_loss: 1.5569\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1882 - val_loss: 1.5493\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1806 - val_loss: 1.5425\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1740 - val_loss: 1.5351\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1713 - val_loss: 1.5279\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1629 - val_loss: 1.5193\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1585 - val_loss: 1.5121\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.1517 - val_loss: 1.5057\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1447 - val_loss: 1.4989\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1406 - val_loss: 1.4936\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1350 - val_loss: 1.4894\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1308 - val_loss: 1.4856\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1300 - val_loss: 1.4807\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1236 - val_loss: 1.4761\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1210 - val_loss: 1.4722\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1170 - val_loss: 1.4678\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1118 - val_loss: 1.4642\n",
      "Epoch 53/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1113 - val_loss: 1.4593\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1048 - val_loss: 1.4556\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1026 - val_loss: 1.4522\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0993 - val_loss: 1.4494\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0948 - val_loss: 1.4461\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0965 - val_loss: 1.4429\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0927 - val_loss: 1.4395\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.0888 - val_loss: 1.4367\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.0877 - val_loss: 1.4334\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0841 - val_loss: 1.4309\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0832 - val_loss: 1.4281\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0810 - val_loss: 1.4258\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0772 - val_loss: 1.4232\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0748 - val_loss: 1.4208\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0761 - val_loss: 1.4184\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0746 - val_loss: 1.4148\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0732 - val_loss: 1.4126\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0697 - val_loss: 1.4109\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0693 - val_loss: 1.4089\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0663 - val_loss: 1.4075\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0640 - val_loss: 1.4054\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0662 - val_loss: 1.4034\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0618 - val_loss: 1.4017\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0600 - val_loss: 1.3996\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0587 - val_loss: 1.3968\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0577 - val_loss: 1.3953\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0567 - val_loss: 1.3939\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0526 - val_loss: 1.3928\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0538 - val_loss: 1.3912\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0555 - val_loss: 1.3898\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0520 - val_loss: 1.3882\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0494 - val_loss: 1.3871\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0513 - val_loss: 1.3854\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0462 - val_loss: 1.3841\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0477 - val_loss: 1.3827\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0441 - val_loss: 1.3813\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0448 - val_loss: 1.3805\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0432 - val_loss: 1.3791\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0420 - val_loss: 1.3778\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0434 - val_loss: 1.3766\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0415 - val_loss: 1.3756\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0411 - val_loss: 1.3742\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0398 - val_loss: 1.3733\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0398 - val_loss: 1.3723\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0389 - val_loss: 1.3711\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0362 - val_loss: 1.3700\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0380 - val_loss: 1.3691\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.0381 - val_loss: 1.3681\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAEICAYAAABGaK+TAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAZ80lEQVR4nO3de7hcdX3v8fcHQg73S2AbAyFsqBAEKpduEbw9SMByT85zLIIKQbFp7dGCpYVgLycebY0ei/AcW9sYIFEoFwMYHqyWGEG0RSBBFELwhEsggVw2l5QEKBD5nj9+vx1WJjN7JnvPzuwf+byeZ569brPWd2bWfOa3fmvNHkUEZmZWnm06XYCZmQ2MA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFAO8EGQtEjScZ2uY6hJ+rKkZyWt7HQtjUg6TtLyFpedJumaQWxrqaQT8rAkXS3pBUn3DnSdw52k8yT9vNN12MYc4A1U36SVaRvtxBFxaETc2WQ93ZJC0oghKnVISRoHXAQcEhFvb9M6Q9Lq6nMiabs8reNfTJC0q6TLJT0laZ2kx/L4XnUWfz9wIjA2Io6WdIykeZKel9Qr6XuSxrSwzVmS1reybGkkTZT0gKQXc0PgJ5L273RdbwUO8MJtgQ+GccBzEbF6c+/YpLYXgJMr4yfnaR0laSQwHzgUOAnYFTgWeA44us5d9gOWRsRLeXwPYAbQneetBa5uss2dgP8B/CfwiUE/iGFE0juA75AaAbsB+wP/APy2jduQpK0zyyLCtzo3YClwQs2084Cf11uG9OZeALwIrAIuy9OfAgJYl2/Hkj44/wp4ElhN2sF3q6z33DzvOeCva7YzDZgDXJO39em87buBNcAK4JvAyMr6AvgTYAkpUL4E/A7wH3kdN1aXr9zvBOAV4I1c+6w8/QxgUd7encA7a56TS4BfA68CI+qsN/Lj/15l2hzgL9MuuWHa3sCtwPPAo8AfVubtAMwihf7DwF8Ay2vuexPQCzwB/Gll3jTgmgav+6fz67dzs30DOB/4L1IYrQO+WGfZo4C1Tfa1c4FlwAXAQzXzpuXX5zv5tVsE9FTmvzO/BmvyvDMq82YB/wj8MNf378Dbgcvz8/YIcGRl+anAY3k7DwP/vd6+Twrgv6+p81bg83Ue20eAB/p57NsCX6hsdyGwb573XuA+0gfbfcB7K/e7E/jb/JheAd4BHAzMy/vLb4AzK8ufkh/TWuBp4M87nTHtuHW8gOF6Y/MD/G7gnDy8M3BMHu4mBdaIyv0+RQqkA/KyNwPfzfMOyW+29wMjga8Dr7NxgL8OTCJ9EOwA/B5wDDAib28xcGFlewHMJbUmDyUF6/y8/d3yjj25wfNwHBsH40HAS6Rug+2Ai/NjGVl5Th4A9gV2aLDOAA4jBeXupFbrqjwtKsvdRQqg7YEjSGF8fJ43HfgZMCpv66G+OvPzshD4m/wcHgA8Dvx+5TlsFODXA7Nb3Tdq94k6y14I/KLJ+uYDXwNGA+uB36vMm0b6kDiFFHZf6Vtffv4fJQXgSOB4UkCNz/NnAc/m/WN74CekD7Nz87q+DNxR2dYfkD74tgE+ml/nMbWPk9RgeAbYJo/vBbwMjK7z2A7I9X8D+BA1H4ykD94HgfGAgMOBPfPr+gJwDmm/PjuP75nvdyepcXRonr8b6UPwk3n8yPzYD8nLrwA+kIf3AI7qdMa049bxAobrLb9J15FaNn23l2kc4HcBXwT2qllPN5sG+HzgTyrj40mhPIIUOtdV5u0IvMbGAX5Xk9ovBG6pjAfwvsr4QuCSyvjfA5c3WNdxbBzgfw3cWBnfhtSiOa7ynHyqSX1BajHNBP4I+GPg23la5GX2JbVsd6nc7yu8eRTwOHBSZd4U3gzw9wBP1WzzUuDqynPYKMDnAdNb2DeaBjjwLlJr8AP9rGsc6QjniDz+b8AVlfnTgB9Xxg8BXsnDHwBWkoM0T7sOmJaHZwHfrsz7HLC4Mv67wJp+ansAmFjvcZIaCSfm4c8C/9rPeo4hHUX0ksJ8FjnISS3liXXucw5wb820u4Hz8vCdwP+uzPso8LOa5f8Z+F95+Km8r+3a32tb2m3r7Ddq3aSI2L3vRuqGaOR8Uuv0EUn3STqtn2X3JnWR9HmSFN6j87xlfTMi4mVSV0rVsuqIpIMk3SZppaQXgb8jtYqqVlWGX6kzvnM/9TasPSLeyPXs06i+fnyH1Bo8Nw/Xbuf5iFhbmfZkZTsbPU9s/HzuB+wtaU3fjdRKHd1CTc8Bgz6RmPt+fwhcEBE/62fRc0ih+kAevxb4mKTtKstUr/55Gdg+n1/YG1iWX4M+1ecINuN1l3RuPtnY95wdxqb7UZ/ZvNlf/wngu40eYET8IiLOjIgu0ofOB0ndZZA+qB+rc7fa9whs+tiqr/9+wHtqXvOPk7qMIJ1jOAV4UtJPJR3bqN6SOMDbJCKWRMTZwNuArwJz8smpqLP4M6Qdrs840qHzKtKh3ti+GZJ2IB1SbrS5mvFvkfozD4yIXUlhpYE/mn5tVLskkd6ET/dTXyM/I4XlaKD2ErVngFGSdqlMG1fZzoq83eq8PsuAJ6ofvhGxS0Sc0kJNPwZ+P792AyJpv7yeL0VEw2DLzgUOyB++K4HLSKHZSq3PAPvWnMCrPkebW/O3Sa3pPXOD5SEa70fXABMlHU7qh/9+K9uJiPtIXYaH5UnLSOdjatW+R2DTx1bdz5YBP615zXeOiM/0bTciJpLen98nHREUzwHeJpI+Iakrt4bW5MlvkA4b3yD1Bfa5Dvi8pP0l7UxqMd8QEetJJ/NOl/TefEXENJqH8S6kk5HrJB0MfKZND6ueG4FTJU3IrcSLSH3q/7G5K4p0bHs66cRb1Mxbltf5FUnbS3oX6Sin7/rtG4FLJe0haSype6DPvcBaSZdI2kHStpIOk/TuFsr6LikMbpJ0sKRtJO0p6QuSmoaqpH1Ifc3fjIh/arLssaTwOprUx38EKdj+hRTszdxDapFfnC/DPI70fF7fwn1r9TU2enNtn+TNkN1ERCwnnVj8LnBTRLxSbzlJ75f0h5LelscPJp0E/0VeZCbwJUkH5qtJ3iVpT+BfgYMkfUzSCEkfJXUf3dagpNvy8ufk52I7Se+W9E5JIyV9XNJuEfE66b3yRoP1FMUB3j4nAYskrQOuAM6KiFdyF8jfAv+eD+2OAa4i7fh3kU4q/Rc5gCJiUR6+ntTKXEe6UuXVfrb958DHSCewvg3c0P6Hl0TEb0iHzP+XdJLodOD0iHhtgOtblB9zPWeTziE8A9xC6s/8cZ73RdIh9RPA7VQO4SPit8BppEB8Itc5k3Siq1k9r5KuMHmE1B/+IukDYS9SYDbzadKH9bR8Dfm6vE/UMxmYGxEPRsTKvhtp/zlN0qgmtb5Gev5Pzo/xH4FzI+KRFuqsXdfDpHMhd5OOBH+XdIVHf2bn5fo7ylhDCuwH8/PwI9Jr+bU8/zLSh/HtpOf6StLJ7+dIr+FFpG6ti4HTIuLZBvWvBT4MnEXaX1aSjoT/W17kHGBp7mL8Y1L3SvFU0/CxYSa30NeQukee6HA5ZhtI+iDpiGi/2iMo2zLcAh+GJJ0uacfcD/t10mVWSztbldmbcvfZBcBMh3fnOMCHp4mkw8BngANJ3TF+k9iwIOmdpKPCMaQvBVmHuAvFzKxQboGbmRVqi/6HvL322iu6u7u35CbNzIq3cOHCZ/MXoTayRQO8u7ubBQsWbMlNmpkVT1Ltt1IBd6GYmRXLAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFahrgksbnn1nqu70o6UJJoyTNk7Qk/91jSxRsZmZJ029i5n/gfwSApG1JP2l0CzAVmB8R0yVNzeOXDF2pQ6N76g82DC+dfmoHKzEz2zyb24UyAXgsIp4k/cvT2Xn6bGBSG+syM7MmNjfAzyL9niPA6IhYkYdX0tovfpuZWZu0HOD5B3bPAL5XOy//2EDdfywuaYqkBZIW9Pb2DrhQMzPb2Oa0wE8G7o+IVXl8laQxAPnv6np3iogZEdETET1dXZv8N0QzMxugzQnws3mz+wTgVtKvapP/zm1XUWZm1lxLAZ5/XPdE4ObK5OnAiZKWACfkcTMz20Ja+kGHiHgJ2LNm2nOkq1LMzKwD/E1MM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzArlADczK5QD3MysUC39M6utRfX3McG/kWlmw5tb4GZmhXKAm5kVygFuZlYoB7iZWaEc4GZmhdoqr0KpvdrEzKxEboGbmRXKAW5mVqiWAlzS7pLmSHpE0mJJx0oaJWmepCX57x5DXayZmb2p1Rb4FcCPIuJg4HBgMTAVmB8RBwLz87iZmW0hTQNc0m7AB4ErASLitYhYA0wEZufFZgOThqZEMzOrp5UW+P5AL3C1pF9KmilpJ2B0RKzIy6wERte7s6QpkhZIWtDb29ueqs3MrKUAHwEcBXwrIo4EXqKmuyQiAoh6d46IGRHRExE9XV1dg63XzMyyVgJ8ObA8Iu7J43NIgb5K0hiA/Hf10JRoZmb1NA3wiFgJLJM0Pk+aADwM3ApMztMmA3OHpEIzM6ur1W9ifg64VtJI4HHgk6Twv1HS+cCTwJlDU6KZmdXTUoBHxANAT51ZE9pajZmZtczfxDQzK5QD3MysUA5wM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzArlADczK5QD3MysUA5wM7NCOcDNzArV0o8aS1oKrAV+C6yPiB5Jo4AbgG5gKXBmRLwwNGUOTPfUH2wYXjr91A5WYmbWfpvTAv9QRBwREX2/Tj8VmB8RBwLz87iZmW0hg+lCmQjMzsOzgUmDrsbMzFrWUhcKEMDtkgL454iYAYyOiBV5/kpgdL07SpoCTAEYN27cIMsduGp3ipnZW0GrAf7+iHha0tuAeZIeqc6MiMjhvokc9jMAenp66i5jZmabr6UulIh4Ov9dDdwCHA2skjQGIP9dPVRFmpnZppoGuKSdJO3SNwx8GHgIuBWYnBebDMwdqiLNzGxTrXShjAZukdS3/L9ExI8k3QfcKOl84EngzKEr08zMajUN8Ih4HDi8zvTngAlDUZSZmTXnb2KamRXKAW5mVigHuJlZoVq9Dnyr5P+lYmbDmVvgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXKAW5mVigHuJlZoVoOcEnbSvqlpNvy+P6S7pH0qKQbJI0cujLNzKzW5rTALwAWV8a/CnwjIt4BvACc387CzMysfy0FuKSxwKnAzDwu4HhgTl5kNjBpCOozM7MGWm2BXw5cDLyRx/cE1kTE+jy+HNin3h0lTZG0QNKC3t7ewdRqZmYVTQNc0mnA6ohYOJANRMSMiOiJiJ6urq6BrMLMzOpo5Vfp3wecIekUYHtgV+AKYHdJI3IrfCzw9NCVaWZmtZq2wCPi0ogYGxHdwFnATyLi48AdwEfyYpOBuUNWpZmZbWIw14FfAvyZpEdJfeJXtqckMzNrRStdKBtExJ3AnXn4ceDo9pdkZmat8DcxzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFAOcDOzQjUNcEnbS7pX0q8kLZL0xTx9f0n3SHpU0g2SRg59uWZm1qeVFvirwPERcThwBHCSpGOArwLfiIh3AC8A5w9ZlWZmtommAR7Jujy6Xb4FcDwwJ0+fDUwaigLNzKy+lvrAJW0r6QFgNTAPeAxYExHr8yLLgX0a3HeKpAWSFvT29rahZDMzgxYDPCJ+GxFHAGOBo4GDW91ARMyIiJ6I6Onq6hpYlWZmtonNugolItYAdwDHArtLGpFnjQWebm9pZmbWnxHNFpDUBbweEWsk7QCcSDqBeQfwEeB6YDIwdygL7bTuqT/YMLx0+qkdrMTMLGka4MAYYLakbUkt9hsj4jZJDwPXS/oy8EvgyiGs08zMajQN8Ij4NXBknemPk/rDzcysA/xNTDOzQjnAzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK1cpvYg5r1R8bBv/gsJltPdwCNzMrVNMAl7SvpDskPSxpkaQL8vRRkuZJWpL/7jH05ZqZWZ9WulDWAxdFxP2SdgEWSpoHnAfMj4jpkqYCU4FLhq7U1tR2qZiZvVU1bYFHxIqIuD8PrwUWA/sAE4HZebHZwKQhqtHMzOrYrD5wSd3AkcA9wOiIWJFnrQRGN7jPFEkLJC3o7e0dTK1mZlbRcoBL2hm4CbgwIl6szouIAKLe/SJiRkT0RERPV1fXoIo1M7M3tRTgkrYjhfe1EXFznrxK0pg8fwywemhKNDOzelq5CkXAlcDiiLisMutWYHIengzMbX95ZmbWSCtXobwPOAd4UNIDedoXgOnAjZLOB54EzhySCs3MrK6mAR4RPwfUYPaE9pZjZmat8jcxzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFAOcDOzQjnAzcwK5QA3MyuUA9zMrFCt/KCD1eie+oMNw0unn9rBSsxsa+YWuJlZoRzgZmaFcoCbmRXKAW5mVqimAS7pKkmrJT1UmTZK0jxJS/LfPYa2TDMzq9VKC3wWcFLNtKnA/Ig4EJifx83MbAtqGuARcRfwfM3kicDsPDwbmNTesszMrJmB9oGPjogVeXglMLpN9ZiZWYsG/UWeiAhJ0Wi+pCnAFIBx48YNdnPDjr/UY2adMtAW+CpJYwDy39WNFoyIGRHRExE9XV1dA9ycmZnVGmiA3wpMzsOTgbntKcfMzFrVymWE1wF3A+MlLZd0PjAdOFHSEuCEPG5mZltQ0z7wiDi7wawJba7FzMw2g7+JaWZWKAe4mVmhHOBmZoVygJuZFcoBbmZWKAe4mVmhHOBmZoVygJuZFcoBbmZWqEH/N0Iz2/oMp//COZxq2dLcAjczK5QD3MysUO5CaaOt+VDOrJnBvj/8/tqUW+BmZoVygJuZFaqYLpTq4ZOZvXUNpqukNifa2dUyHLtw3AI3MyuUA9zMrFDFdKGYvdUMx0PyWp3uuhzs9jv5HG+JbbsFbmZWKAe4mVmhBtWFIukk4ApgW2BmRExvS1VmBRuuXSP9dUdU62xnt8Vglmm3zX1dWq2xk6/3gFvgkrYF/gE4GTgEOFvSIe0qzMzM+jeYLpSjgUcj4vGIeA24HpjYnrLMzKwZRcTA7ih9BDgpIj6dx88B3hMRn61ZbgowJY+OB34z8HL7tRfw7BCtu51cZ3u5zvYrpdatqc79IqKrduKQX0YYETOAGUO9HUkLIqJnqLczWK6zvVxn+5VSq+scXBfK08C+lfGxeZqZmW0Bgwnw+4ADJe0vaSRwFnBre8oyM7NmBtyFEhHrJX0W+DfSZYRXRcSitlW2+Ya8m6ZNXGd7uc72K6XWrb7OAZ/ENDOzzvI3Mc3MCuUANzMrVNEBLmlfSXdIeljSIkkXdLqm/kjaVtIvJd3W6Vr6I2l3SXMkPSJpsaRjO11TPZI+n1/3hyRdJ2n7TtcEIOkqSaslPVSZNkrSPElL8t89Olljrqlenf8nv+6/lnSLpN07WOIG9WqtzLtIUkjaqxO11dRSt05Jn8vP6yJJX2vX9ooOcGA9cFFEHAIcA/zPYf51/guAxZ0uogVXAD+KiIOBwxmGNUvaB/hToCciDiOdSD+rs1VtMAs4qWbaVGB+RBwIzM/jnTaLTeucBxwWEe8C/h9w6ZYuqoFZbForkvYFPgw8taULamAWNXVK+hDpW+qHR8ShwNfbtbGiAzwiVkTE/Xl4LSlo9ulsVfVJGgucCszsdC39kbQb8EHgSoCIeC0i1nS0qMZGADtIGgHsCDzT4XoAiIi7gOdrJk8EZufh2cCkLVlTPfXqjIjbI2J9Hv0F6fsdHdfgOQX4BnAxMCyuxmhQ52eA6RHxal5mdbu2V3SAV0nqBo4E7ulwKY1cTtrR3uhwHc3sD/QCV+funpmSdup0UbUi4mlSS+YpYAXwnxFxe2er6tfoiFiRh1cCoztZTIs+Bfyw00U0Imki8HRE/KrTtTRxEPABSfdI+qmkd7drxW+JAJe0M3ATcGFEvNjpempJOg1YHRELO11LC0YARwHfiogjgZcYHof7G8l9yBNJHzh7AztJ+kRnq2pNpGt3h0WLsRFJf0nqory207XUI2lH4AvA33S6lhaMAEaRunn/ArhRktqx4uIDXNJ2pPC+NiJu7nQ9DbwPOEPSUtJ/bTxe0jWdLamh5cDyiOg7kplDCvTh5gTgiYjojYjXgZuB93a4pv6skjQGIP9t22F0u0k6DzgN+HgM3y+K/A7pw/tX+X01Frhf0ts7WlV9y4GbI7mXdBTelhOuRQd4/hS7ElgcEZd1up5GIuLSiBgbEd2kE20/iYhh2VqMiJXAMknj86QJwMMdLKmRp4BjJO2Y94MJDMOTrRW3ApPz8GRgbgdraSj/SMvFwBkR8XKn62kkIh6MiLdFRHd+Xy0Hjsr773DzfeBDAJIOAkbSpv+iWHSAk1q255BatA/k2ymdLuot4HPAtZJ+DRwB/F1ny9lUPkKYA9wPPEjal4fFV6slXQfcDYyXtFzS+cB04ERJS0hHDx3/9aoGdX4T2AWYl99P/9TRIrMGtQ47Deq8CjggX1p4PTC5XUc2/iq9mVmhSm+Bm5lttRzgZmaFcoCbmRXKAW5mVigHuJlZoRzgZmaFcoCbmRXq/wP89bT+MrJQuAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "clf2 = AutoEncoder(hidden_neurons =[25, 10,2, 10, 25])\n",
    "clf2.fit(X_train)\n",
    "\n",
    "# Predict the anomaly scores\n",
    "y_test_scores = clf2.decision_function(X_test)  \n",
    "y_test_scores = pd.Series(y_test_scores)\n",
    "\n",
    "# Plot the histogram\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_test_scores, bins='auto')  \n",
    "plt.title(\"Histogram for Model Clf2 Anomaly Scores\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "## choose 4.0 to be the cut point and those >=4.0 to be outliers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T11:49:43.088769Z",
     "iopub.status.busy": "2021-07-15T11:49:43.088431Z",
     "iopub.status.idle": "2021-07-15T11:49:43.121029Z",
     "shell.execute_reply": "2021-07-15T11:49:43.120336Z",
     "shell.execute_reply.started": "2021-07-15T11:49:43.088741Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.038355</td>\n",
       "      <td>2.969163</td>\n",
       "      <td>2.995461</td>\n",
       "      <td>3.003440</td>\n",
       "      <td>2.981764</td>\n",
       "      <td>2.986800</td>\n",
       "      <td>2.992153</td>\n",
       "      <td>2.970264</td>\n",
       "      <td>3.059222</td>\n",
       "      <td>2.996335</td>\n",
       "      <td>...</td>\n",
       "      <td>3.015313</td>\n",
       "      <td>3.007682</td>\n",
       "      <td>3.060115</td>\n",
       "      <td>2.993295</td>\n",
       "      <td>2.946253</td>\n",
       "      <td>3.001370</td>\n",
       "      <td>3.028487</td>\n",
       "      <td>2.971585</td>\n",
       "      <td>3.001477</td>\n",
       "      <td>2.705108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.129767</td>\n",
       "      <td>0.231696</td>\n",
       "      <td>-0.225120</td>\n",
       "      <td>0.273774</td>\n",
       "      <td>0.228626</td>\n",
       "      <td>-0.125085</td>\n",
       "      <td>-0.139579</td>\n",
       "      <td>0.282078</td>\n",
       "      <td>-0.057665</td>\n",
       "      <td>-0.120514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111293</td>\n",
       "      <td>-0.060746</td>\n",
       "      <td>-0.082725</td>\n",
       "      <td>-0.036849</td>\n",
       "      <td>0.394688</td>\n",
       "      <td>0.339425</td>\n",
       "      <td>0.250229</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>-0.093264</td>\n",
       "      <td>13.474216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "cluster                                                                         \n",
       "0        3.038355  2.969163  2.995461  3.003440  2.981764  2.986800  2.992153   \n",
       "1        0.129767  0.231696 -0.225120  0.273774  0.228626 -0.125085 -0.139579   \n",
       "\n",
       "                7         8         9  ...        16        17        18  \\\n",
       "cluster                                ...                                 \n",
       "0        2.970264  3.059222  2.996335  ...  3.015313  3.007682  3.060115   \n",
       "1        0.282078 -0.057665 -0.120514  ...  0.111293 -0.060746 -0.082725   \n",
       "\n",
       "               19        20        21        22        23        24      score  \n",
       "cluster                                                                         \n",
       "0        2.993295  2.946253  3.001370  3.028487  2.971585  3.001477   2.705108  \n",
       "1       -0.036849  0.394688  0.339425  0.250229  0.035533 -0.093264  13.474216  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = X_test.copy()\n",
    "df_test['score'] = y_test_scores\n",
    "df_test['cluster'] = np.where(df_test['score']<4, 0, 1)\n",
    "df_test['cluster'].value_counts()\n",
    "df_test.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PYOD model 3 ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T11:50:17.508515Z",
     "iopub.status.busy": "2021-07-15T11:50:17.507977Z",
     "iopub.status.idle": "2021-07-15T11:50:28.59587Z",
     "shell.execute_reply": "2021-07-15T11:50:28.594965Z",
     "shell.execute_reply.started": "2021-07-15T11:50:17.508484Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_96 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_49 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_97 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_50 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_98 (Dense)            (None, 25)                650       \n",
      "                                                                 \n",
      " dropout_51 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_99 (Dense)            (None, 15)                390       \n",
      "                                                                 \n",
      " dropout_52 (Dropout)        (None, 15)                0         \n",
      "                                                                 \n",
      " dense_100 (Dense)           (None, 10)                160       \n",
      "                                                                 \n",
      " dropout_53 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_101 (Dense)           (None, 2)                 22        \n",
      "                                                                 \n",
      " dropout_54 (Dropout)        (None, 2)                 0         \n",
      "                                                                 \n",
      " dense_102 (Dense)           (None, 10)                30        \n",
      "                                                                 \n",
      " dropout_55 (Dropout)        (None, 10)                0         \n",
      "                                                                 \n",
      " dense_103 (Dense)           (None, 15)                165       \n",
      "                                                                 \n",
      " dropout_56 (Dropout)        (None, 15)                0         \n",
      "                                                                 \n",
      " dense_104 (Dense)           (None, 25)                400       \n",
      "                                                                 \n",
      " dropout_57 (Dropout)        (None, 25)                0         \n",
      "                                                                 \n",
      " dense_105 (Dense)           (None, 25)                650       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,767\n",
      "Trainable params: 3,767\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "15/15 [==============================] - 1s 23ms/step - loss: 4.3281 - val_loss: 3.9991\n",
      "Epoch 2/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 3.3978 - val_loss: 3.3518\n",
      "Epoch 3/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.9183 - val_loss: 2.9751\n",
      "Epoch 4/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.6214 - val_loss: 2.7288\n",
      "Epoch 5/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.3938 - val_loss: 2.5315\n",
      "Epoch 6/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 2.2081 - val_loss: 2.3700\n",
      "Epoch 7/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 2.0804 - val_loss: 2.2368\n",
      "Epoch 8/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.9654 - val_loss: 2.1276\n",
      "Epoch 9/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.8613 - val_loss: 2.0394\n",
      "Epoch 10/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.7837 - val_loss: 1.9657\n",
      "Epoch 11/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.7137 - val_loss: 1.9028\n",
      "Epoch 12/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.6583 - val_loss: 1.8477\n",
      "Epoch 13/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.6056 - val_loss: 1.8040\n",
      "Epoch 14/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.5631 - val_loss: 1.7625\n",
      "Epoch 15/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.5234 - val_loss: 1.7295\n",
      "Epoch 16/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.4849 - val_loss: 1.6997\n",
      "Epoch 17/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.4656 - val_loss: 1.6725\n",
      "Epoch 18/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.4347 - val_loss: 1.6474\n",
      "Epoch 19/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.4148 - val_loss: 1.6258\n",
      "Epoch 20/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.3959 - val_loss: 1.6059\n",
      "Epoch 21/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.3715 - val_loss: 1.5862\n",
      "Epoch 22/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.3532 - val_loss: 1.5683\n",
      "Epoch 23/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.3351 - val_loss: 1.5522\n",
      "Epoch 24/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.3198 - val_loss: 1.5371\n",
      "Epoch 25/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.3112 - val_loss: 1.5232\n",
      "Epoch 26/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.2936 - val_loss: 1.5093\n",
      "Epoch 27/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2752 - val_loss: 1.4961\n",
      "Epoch 28/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.2728 - val_loss: 1.4836\n",
      "Epoch 29/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2598 - val_loss: 1.4729\n",
      "Epoch 30/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2480 - val_loss: 1.4630\n",
      "Epoch 31/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2387 - val_loss: 1.4528\n",
      "Epoch 32/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.2354 - val_loss: 1.4436\n",
      "Epoch 33/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.2252 - val_loss: 1.4355\n",
      "Epoch 34/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.2151 - val_loss: 1.4278\n",
      "Epoch 35/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.2103 - val_loss: 1.4195\n",
      "Epoch 36/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1987 - val_loss: 1.4117\n",
      "Epoch 37/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1908 - val_loss: 1.4049\n",
      "Epoch 38/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.1916 - val_loss: 1.3986\n",
      "Epoch 39/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.1784 - val_loss: 1.3924\n",
      "Epoch 40/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1729 - val_loss: 1.3865\n",
      "Epoch 41/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1702 - val_loss: 1.3809\n",
      "Epoch 42/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1654 - val_loss: 1.3755\n",
      "Epoch 43/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1575 - val_loss: 1.3707\n",
      "Epoch 44/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1562 - val_loss: 1.3658\n",
      "Epoch 45/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1486 - val_loss: 1.3610\n",
      "Epoch 46/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.1472 - val_loss: 1.3565\n",
      "Epoch 47/100\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 1.1403 - val_loss: 1.3520\n",
      "Epoch 48/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1399 - val_loss: 1.3470\n",
      "Epoch 49/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1326 - val_loss: 1.3432\n",
      "Epoch 50/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1287 - val_loss: 1.3399\n",
      "Epoch 51/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1261 - val_loss: 1.3366\n",
      "Epoch 52/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1197 - val_loss: 1.3331\n",
      "Epoch 53/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1227 - val_loss: 1.3301\n",
      "Epoch 54/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1186 - val_loss: 1.3268\n",
      "Epoch 55/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.1137 - val_loss: 1.3238\n",
      "Epoch 56/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.1104 - val_loss: 1.3212\n",
      "Epoch 57/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1098 - val_loss: 1.3186\n",
      "Epoch 58/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1054 - val_loss: 1.3159\n",
      "Epoch 59/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1076 - val_loss: 1.3133\n",
      "Epoch 60/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.1025 - val_loss: 1.3106\n",
      "Epoch 61/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.1020 - val_loss: 1.3082\n",
      "Epoch 62/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0959 - val_loss: 1.3061\n",
      "Epoch 63/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0962 - val_loss: 1.3039\n",
      "Epoch 64/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0932 - val_loss: 1.3017\n",
      "Epoch 65/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0891 - val_loss: 1.2989\n",
      "Epoch 66/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0903 - val_loss: 1.2969\n",
      "Epoch 67/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0899 - val_loss: 1.2951\n",
      "Epoch 68/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0903 - val_loss: 1.2935\n",
      "Epoch 69/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0832 - val_loss: 1.2916\n",
      "Epoch 70/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.0829 - val_loss: 1.2900\n",
      "Epoch 71/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0836 - val_loss: 1.2883\n",
      "Epoch 72/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0836 - val_loss: 1.2867\n",
      "Epoch 73/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.0789 - val_loss: 1.2850\n",
      "Epoch 74/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0785 - val_loss: 1.2835\n",
      "Epoch 75/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0746 - val_loss: 1.2821\n",
      "Epoch 76/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0756 - val_loss: 1.2806\n",
      "Epoch 77/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0712 - val_loss: 1.2792\n",
      "Epoch 78/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0729 - val_loss: 1.2779\n",
      "Epoch 79/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0699 - val_loss: 1.2766\n",
      "Epoch 80/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0692 - val_loss: 1.2752\n",
      "Epoch 81/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0683 - val_loss: 1.2741\n",
      "Epoch 82/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0685 - val_loss: 1.2727\n",
      "Epoch 83/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0673 - val_loss: 1.2716\n",
      "Epoch 84/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0666 - val_loss: 1.2702\n",
      "Epoch 85/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0651 - val_loss: 1.2691\n",
      "Epoch 86/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0634 - val_loss: 1.2680\n",
      "Epoch 87/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0603 - val_loss: 1.2669\n",
      "Epoch 88/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0612 - val_loss: 1.2658\n",
      "Epoch 89/100\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.0586 - val_loss: 1.2647\n",
      "Epoch 90/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0599 - val_loss: 1.2638\n",
      "Epoch 91/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0606 - val_loss: 1.2630\n",
      "Epoch 92/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0572 - val_loss: 1.2619\n",
      "Epoch 93/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0591 - val_loss: 1.2609\n",
      "Epoch 94/100\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.0556 - val_loss: 1.2601\n",
      "Epoch 95/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0567 - val_loss: 1.2587\n",
      "Epoch 96/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0536 - val_loss: 1.2578\n",
      "Epoch 97/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0561 - val_loss: 1.2571\n",
      "Epoch 98/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0528 - val_loss: 1.2562\n",
      "Epoch 99/100\n",
      "15/15 [==============================] - 0s 4ms/step - loss: 1.0515 - val_loss: 1.2554\n",
      "Epoch 100/100\n",
      "15/15 [==============================] - 0s 3ms/step - loss: 1.0544 - val_loss: 1.2547\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.038355</td>\n",
       "      <td>2.969163</td>\n",
       "      <td>2.995461</td>\n",
       "      <td>3.003440</td>\n",
       "      <td>2.981764</td>\n",
       "      <td>2.986800</td>\n",
       "      <td>2.992153</td>\n",
       "      <td>2.970264</td>\n",
       "      <td>3.059222</td>\n",
       "      <td>2.996335</td>\n",
       "      <td>...</td>\n",
       "      <td>3.015313</td>\n",
       "      <td>3.007682</td>\n",
       "      <td>3.060115</td>\n",
       "      <td>2.993295</td>\n",
       "      <td>2.946253</td>\n",
       "      <td>3.001370</td>\n",
       "      <td>3.028487</td>\n",
       "      <td>2.971585</td>\n",
       "      <td>3.001477</td>\n",
       "      <td>2.696991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.129767</td>\n",
       "      <td>0.231696</td>\n",
       "      <td>-0.225120</td>\n",
       "      <td>0.273774</td>\n",
       "      <td>0.228626</td>\n",
       "      <td>-0.125085</td>\n",
       "      <td>-0.139579</td>\n",
       "      <td>0.282078</td>\n",
       "      <td>-0.057665</td>\n",
       "      <td>-0.120514</td>\n",
       "      <td>...</td>\n",
       "      <td>0.111293</td>\n",
       "      <td>-0.060746</td>\n",
       "      <td>-0.082725</td>\n",
       "      <td>-0.036849</td>\n",
       "      <td>0.394688</td>\n",
       "      <td>0.339425</td>\n",
       "      <td>0.250229</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>-0.093264</td>\n",
       "      <td>13.497363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0         1         2         3         4         5         6  \\\n",
       "cluster                                                                         \n",
       "0        3.038355  2.969163  2.995461  3.003440  2.981764  2.986800  2.992153   \n",
       "1        0.129767  0.231696 -0.225120  0.273774  0.228626 -0.125085 -0.139579   \n",
       "\n",
       "                7         8         9  ...        16        17        18  \\\n",
       "cluster                                ...                                 \n",
       "0        2.970264  3.059222  2.996335  ...  3.015313  3.007682  3.060115   \n",
       "1        0.282078 -0.057665 -0.120514  ...  0.111293 -0.060746 -0.082725   \n",
       "\n",
       "               19        20        21        22        23        24      score  \n",
       "cluster                                                                         \n",
       "0        2.993295  2.946253  3.001370  3.028487  2.971585  3.001477   2.696991  \n",
       "1       -0.036849  0.394688  0.339425  0.250229  0.035533 -0.093264  13.497363  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Build the model\n",
    "clf3 = AutoEncoder(hidden_neurons =[25, 15, 10, 2, 10,15, 25])\n",
    "clf3.fit(X_train)\n",
    "\n",
    "# Predict the anomaly scores\n",
    "y_test_scores = clf3.decision_function(X_test)  \n",
    "y_test_scores = pd.Series(y_test_scores)\n",
    "\n",
    "# Step 2: Determine the cut point\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_test_scores, bins='auto')  \n",
    "plt.title(\"Histogram with Model Clf3 Anomaly Scores\")\n",
    "plt.show()\n",
    "\n",
    "df_test = X_test.copy()\n",
    "df_test['score'] = y_test_scores\n",
    "df_test['cluster'] = np.where(df_test['score']<4, 0, 1)\n",
    "df_test['cluster'].value_counts()\n",
    "\n",
    "# Step 3: Get the summary statistics by cluster\n",
    "df_test.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nAlthough unsupervised techniques are powerful in detecting outliers, they are prone to overfitting and unstable results. The solution is to train multiple models then aggregate the scores.\\n\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Although unsupervised techniques are powerful in detecting outliers, they are prone to overfitting and unstable results. The solution is to train multiple models then aggregate the scores.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThere are four methods to aggregate the outcome as below.\\nAverage: average scores of all detectors.\\nMaximum of Maximum (MOM)\\nAverage of Maximum (AOM)\\nMaximum of Average (MOA)\\n\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "There are four methods to aggregate the outcome as below.\n",
    "Average: average scores of all detectors.\n",
    "Maximum of Maximum (MOM)\n",
    "Average of Maximum (AOM)\n",
    "Maximum of Average (MOA)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T11:56:40.318537Z",
     "iopub.status.busy": "2021-07-15T11:56:40.318163Z",
     "iopub.status.idle": "2021-07-15T11:56:40.485792Z",
     "shell.execute_reply": "2021-07-15T11:56:40.484865Z",
     "shell.execute_reply.started": "2021-07-15T11:56:40.318509Z"
    }
   },
   "outputs": [],
   "source": [
    "##### Average Method  #####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T12:02:01.35566Z",
     "iopub.status.busy": "2021-07-15T12:02:01.355293Z",
     "iopub.status.idle": "2021-07-15T12:02:01.502723Z",
     "shell.execute_reply": "2021-07-15T12:02:01.501824Z",
     "shell.execute_reply.started": "2021-07-15T12:02:01.35563Z"
    }
   },
   "outputs": [],
   "source": [
    "# Combination by max\n",
    "# Put all the predictions in a data frame\n",
    "from pyod.models.combination import aom, moa, average, maximization\n",
    "from pyod.utils.utility import standardizer\n",
    "\n",
    "# Put all the predictions in a data frame\n",
    "train_scores = pd.DataFrame({'clf1': clf1.decision_scores_,\n",
    "                             'clf2': clf2.decision_scores_,\n",
    "                             'clf3': clf3.decision_scores_\n",
    "                            })\n",
    "\n",
    "test_scores  = pd.DataFrame({'clf1': clf1.decision_function(X_test),\n",
    "                             'clf2': clf2.decision_function(X_test),\n",
    "                             'clf3': clf3.decision_function(X_test) \n",
    "                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T12:02:05.282466Z",
     "iopub.status.busy": "2021-07-15T12:02:05.282064Z",
     "iopub.status.idle": "2021-07-15T12:02:05.292393Z",
     "shell.execute_reply": "2021-07-15T12:02:05.291338Z",
     "shell.execute_reply.started": "2021-07-15T12:02:05.282436Z"
    }
   },
   "outputs": [],
   "source": [
    "# Although we did standardization before, it was for the variables.\n",
    "# Now we do the standardization for the decision scores\n",
    "train_scores_norm, test_scores_norm = standardizer(train_scores,test_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T12:02:24.011324Z",
     "iopub.status.busy": "2021-07-15T12:02:24.010977Z",
     "iopub.status.idle": "2021-07-15T12:02:24.014753Z",
     "shell.execute_reply": "2021-07-15T12:02:24.013904Z",
     "shell.execute_reply.started": "2021-07-15T12:02:24.011295Z"
    }
   },
   "outputs": [],
   "source": [
    "########### Average Method ############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T12:02:33.613527Z",
     "iopub.status.busy": "2021-07-15T12:02:33.613162Z",
     "iopub.status.idle": "2021-07-15T12:02:34.213078Z",
     "shell.execute_reply": "2021-07-15T12:02:34.212157Z",
     "shell.execute_reply.started": "2021-07-15T12:02:33.613497Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combination by average\n",
    "y_by_average = average(test_scores_norm)\n",
    "             \n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_by_average, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Combination by average\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T12:04:08.177029Z",
     "iopub.status.busy": "2021-07-15T12:04:08.176539Z",
     "iopub.status.idle": "2021-07-15T12:04:08.180358Z",
     "shell.execute_reply": "2021-07-15T12:04:08.179506Z",
     "shell.execute_reply.started": "2021-07-15T12:04:08.176997Z"
    }
   },
   "outputs": [],
   "source": [
    "#### e can identify those >=0.0 as the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T12:02:54.097035Z",
     "iopub.status.busy": "2021-07-15T12:02:54.096556Z",
     "iopub.status.idle": "2021-07-15T12:02:54.106755Z",
     "shell.execute_reply": "2021-07-15T12:02:54.105954Z",
     "shell.execute_reply.started": "2021-07-15T12:02:54.097005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    450\n",
       "1     50\n",
       "Name: y_by_average_cluster, dtype: int64"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(X_test)\n",
    "df_test['y_by_average_score'] = y_by_average\n",
    "df_test['y_by_average_cluster'] = np.where(df_test['y_by_average_score']<0, 0, 1)\n",
    "df_test['y_by_average_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Maximum of Maximum Method ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T12:04:46.041296Z",
     "iopub.status.busy": "2021-07-15T12:04:46.040761Z",
     "iopub.status.idle": "2021-07-15T12:04:46.354645Z",
     "shell.execute_reply": "2021-07-15T12:04:46.353952Z",
     "shell.execute_reply.started": "2021-07-15T12:04:46.041265Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Combination by max\n",
    "y_by_maximization = maximization(test_scores_norm)\n",
    "             \n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_by_maximization, bins='auto')  # arguments are passed to np.histogram\n",
    "plt.title(\"Combination by max\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T12:05:25.405717Z",
     "iopub.status.busy": "2021-07-15T12:05:25.405347Z",
     "iopub.status.idle": "2021-07-15T12:05:25.417478Z",
     "shell.execute_reply": "2021-07-15T12:05:25.416421Z",
     "shell.execute_reply.started": "2021-07-15T12:05:25.405683Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    450\n",
       "1     50\n",
       "Name: y_by_maximization_cluster, dtype: int64"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(X_test)\n",
    "df_test['y_by_maximization_score'] = y_by_maximization\n",
    "df_test['y_by_maximization_cluster'] = np.where(df_test['y_by_maximization_score']<0, 0, 1)\n",
    "df_test['y_by_maximization_cluster'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-07-15T12:05:37.864456Z",
     "iopub.status.busy": "2021-07-15T12:05:37.864076Z",
     "iopub.status.idle": "2021-07-15T12:05:37.895232Z",
     "shell.execute_reply": "2021-07-15T12:05:37.894337Z",
     "shell.execute_reply.started": "2021-07-15T12:05:37.864422Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>y_by_average_score</th>\n",
       "      <th>y_by_average_cluster</th>\n",
       "      <th>y_by_maximization_score</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y_by_maximization_cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.038355</td>\n",
       "      <td>2.969163</td>\n",
       "      <td>2.995461</td>\n",
       "      <td>3.003440</td>\n",
       "      <td>2.981764</td>\n",
       "      <td>2.986800</td>\n",
       "      <td>2.992153</td>\n",
       "      <td>2.970264</td>\n",
       "      <td>3.059222</td>\n",
       "      <td>2.996335</td>\n",
       "      <td>...</td>\n",
       "      <td>3.060115</td>\n",
       "      <td>2.993295</td>\n",
       "      <td>2.946253</td>\n",
       "      <td>3.001370</td>\n",
       "      <td>3.028487</td>\n",
       "      <td>2.971585</td>\n",
       "      <td>3.001477</td>\n",
       "      <td>-0.331740</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.328157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.129767</td>\n",
       "      <td>0.231696</td>\n",
       "      <td>-0.225120</td>\n",
       "      <td>0.273774</td>\n",
       "      <td>0.228626</td>\n",
       "      <td>-0.125085</td>\n",
       "      <td>-0.139579</td>\n",
       "      <td>0.282078</td>\n",
       "      <td>-0.057665</td>\n",
       "      <td>-0.120514</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.082725</td>\n",
       "      <td>-0.036849</td>\n",
       "      <td>0.394688</td>\n",
       "      <td>0.339425</td>\n",
       "      <td>0.250229</td>\n",
       "      <td>0.035533</td>\n",
       "      <td>-0.093264</td>\n",
       "      <td>2.952359</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.954828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows  28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  0         1         2         3         4  \\\n",
       "y_by_maximization_cluster                                                     \n",
       "0                          3.038355  2.969163  2.995461  3.003440  2.981764   \n",
       "1                          0.129767  0.231696 -0.225120  0.273774  0.228626   \n",
       "\n",
       "                                  5         6         7         8         9  \\\n",
       "y_by_maximization_cluster                                                     \n",
       "0                          2.986800  2.992153  2.970264  3.059222  2.996335   \n",
       "1                         -0.125085 -0.139579  0.282078 -0.057665 -0.120514   \n",
       "\n",
       "                           ...        18        19        20        21  \\\n",
       "y_by_maximization_cluster  ...                                           \n",
       "0                          ...  3.060115  2.993295  2.946253  3.001370   \n",
       "1                          ... -0.082725 -0.036849  0.394688  0.339425   \n",
       "\n",
       "                                 22        23        24  y_by_average_score  \\\n",
       "y_by_maximization_cluster                                                     \n",
       "0                          3.028487  2.971585  3.001477           -0.331740   \n",
       "1                          0.250229  0.035533 -0.093264            2.952359   \n",
       "\n",
       "                           y_by_average_cluster  y_by_maximization_score  \n",
       "y_by_maximization_cluster                                                 \n",
       "0                                           0.0                -0.328157  \n",
       "1                                           1.0                 2.954828  \n",
       "\n",
       "[2 rows x 28 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.groupby('y_by_maximization_cluster').mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
